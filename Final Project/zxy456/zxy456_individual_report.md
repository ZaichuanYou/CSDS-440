### CSDS 440 Final Project Individual Writeup

**Zaichuan You**

# Survey
For this project, I read the papers *"Multiple instance classification: Review, taxonomy and comparative study"* (J. Amores, 2013), *"Multiple instance learning with bag dissimilarities* (Veronika and David, 2015), and *"Support Vector Machines for Multi ple-Instance Learning"* (Stuart, Ioannis, and Thomas, 2002).

## Multiple instance classification: Review, taxonomy and comparative study (J. Amores, 2013)
This is the seed paper send out by Prof. Ray which give a insight of what is Multi instance learning. It has included the basis of Multiple instance learning and also different approaches that people has made to solve Multi instance learning problem. The basic assumption that all Multi instance learning problem share is:

- Bags are labeled either positive or negative and the label for each instance was not give.
- If a Bag contains at least one positive instance then it will be labeled positive, negative otherwise.

In mathmatical form, Multi instance learning problem can be discribe using Bag $X_i$ and instance $x_j \in X_i$. For ${x_1 .... x_j} \in X_i$ if any $x_n \in X_i$ is positive, $Y_i = 1$

This kind of problem was driven by the real world challenge of classifing images and molecues. In the paper Paulo has included few example to illustrate the basic setup for these problems. One classic example of Multi instance learning problem is the image of beach. In this example, the whole image was a Bag of input. Each instance was generated by kernal sliding through the image. The instance in this case was circled with red lines.

![WeChat Image_20221207215946](https://user-images.githubusercontent.com/89466889/206345550-32579999-3e46-44fa-acba-5df4bcd78b39.png)

The intuitive idea from this example is that it does not make sense if the classifier classify the second image as beach and said "I think this image is beach because it has a blue sky". In this case we know the whole Image was labeled as positive which indicates that it is beach but we can not tell which part of the image is the instance that tells us this is a beach.

This can also be simplify to instance space which we have only a few features instead of kernals as instance. In this example we can clearly see that each bag contains a selection of instance from the instance space. For each bag, if it contains at least one instance from the positive class the whole bag will be classified as positive.

![WeChat Image_20221207220522](https://user-images.githubusercontent.com/89466889/206346368-583915a5-ad2c-4c94-9d27-6aff4c833eea.png)

### Bag distance approach

In their work Paulo named the approaches which focus on learning the true label of each bag as Bag space approach. In Bag space approach researcher want their program to learn a decision boundary which can classify the true label of each bag. There is another paradigm called Embedded space but we will not talk about that today. 

In Bag space approaches, a few of them are focusing on the distance between each bag and using that as an indicator of the true label of each bag. One of the algorithm I have implmented from Veronika and David was employing this method. The bag distancce approach is turning the origional input dataset into a distance matrix which columns and rows are bags. 

![WeChat Image_20221207184027](https://user-images.githubusercontent.com/89466889/206347335-4e98d886-3cb5-439e-8643-68dc00d52273.png)

## Multiple instance learning with bag dissimilarities (Veronika and David, 2015)
This paper discribed the dissimilarities between bags which is calculated using the distance between bags. Bag dissimilarities is an extention of Citation-k NN proposed in 2014 by Veronika Cheplygina. It converge the Multi instance learning task to a normal learning problem by reducing the dimension of data. 

For Multi instance learning problem, instead of getting a dataset which is in standard feature vector representation such that regular supervised classifiers can be used, we we given the data in bag representation which does not include the true label for each instance in the positive bag. This stoping us from employing the regular supervised classifiers. 

The process of transforming original dataset into dissimilarities representation can be view as follow:
- compute a n\*n matrix M where n is the number of bags in the dataset
- $n_{ij} \in M$ denotes the dissimilarity between bag i and bag j

![WeChat Image_20221207223841](https://user-images.githubusercontent.com/89466889/206350859-56db7333-f836-48fa-9c7f-1163416915fa.png)

After transformation each row i can now be view as the list of feature for bag $B_i$ and label for bag $B_i$ is just $Y_i$. Now this matrix can be learned using regular learning approach such as Logistic Regression. 

Based on differen type of problems been given, the distribution of instance will change and we have to use different method to compute the dissimilarites between bags. For instance space which data given was a point set, dissimilarities is defined through the distance between bags. For distribution as instance, dissimilarities is defined though a distribution distance. For attribute graph where the instances are the nodes, andrelationships between instances are the edges. In this case,d is defined as a graph kernel or distance.

Since the dataset Musk1, Musk2, Elephant are all instance space dataset, I will only explain and implement the instance space approach in my project.

The approach of computing the distance between bags can be seen as follow:
- pick the interested instance in first bag $B_i$
- pick the interested instance in the second bag $B_j$
- find the distance between the two instance interested and use as $n_{ij}$

The method been employed here to calculate the distance is Hausdorff distance which $d_h(B_i, B_j)=\max_k\min_l d(X_{i,k}, X_{j,l})$. As $d_h$ is not symetric, the final Hausdorff distance is symmetrized by taking the maximum of the directed distances. All of these steps ensure that the Hausdorff distance is metric. The figure below showed the asymmetric property of $d_h$

![WeChat Image_20221207182642](https://user-images.githubusercontent.com/89466889/206355444-a0b328be-480d-440a-becc-63bbc01600bc.png)

However, Hausdorff distance is highly sensitive to outliers. To mitigate the effect of outlisers, the following change has been made upon Hausdorff distance:
$$d_{minmin}(B_i, B_j) = \min_k\min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmin}(B_i, B_j) = \frac{1}{n_i}\sum_{k=1}^{n_i} \min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmean}(B_i, B_j) = \frac{1}{n_i n_j}\sum_{k=1}^{n_i} \sum_{l=1}^{n_j} d(X_{i,k}, X_{j,l})$$

$d(X_{i,k}, X_{j,l})$ in this case is the Euclidean distance between $X_{i,k}$ and $X_{j,l}$ which is:
$$d(p,q)=\sqrt{\sum_{i=1}^n(q_i-p_i)^2}$$

After conversion, the input matrix will be turn into dissimilarity matrix which is n\*n

## Support Vector Machines for Multi ple-Instance Learning (Stuart, Ioannis, and Thomas, 2002)
This approach tries to learn the true label for each instance. To do that they used the imbalanced information provided by positive and negative bags to train the classifier. They will keep correcting the label of instance in positive bags using the instance from negative bag until there exist at least one instance in each positive bag which been classified as positive. 

### mi_SVM

$$\min_{y_i}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall i:y_i(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0, y_i \in {-1,1}$

Notice in a regular learning task the label $y_i$ will be given but for Multi instance learning taks label for $x_i$ from positive bag is an unknow variable. In mi-SVM one thus maximizes a soft-margin criterion jointly over possible label assignments as well as hyperplanes.

After wards this will be used to optimize the decision boundary and I will mention that in method section.

### MI_SVM

$$\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_I\xi_I$$

s.t. $\forall I:Y_I\max_{i\in{I}}(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=1$

$\forall i:-<w,x_i>+b \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=-1$

For positive bag we will introduce a selet variable $s(I)\in I$ denote the pattern been selected. Then we can rewrite the equation as:

$$\min_{s}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall I:Y_I\=-1 \wedge -<w,x_i>+b \geq 1-\xi_i, \forall i \in I$,

or, $\forall I:Y_I\=1 \wedge <w,x_i>+b \geq 1-\xi_i,$ and $\xi_I \geq 0$

## Comparason
As a whole, Multiple instance learning with bag dissimilarities out performed Support Vector Machines for Multi ple-Instance Learning. This is because leaning the specific label assigned to each instance could be more complecated than simply leang from bag label. With continuous making assumption of the label of instance inside the positive bag, it is likely multi instance support vector machine itself will create noise in thie process. The larger the dataset is, multi instance support vector machine is more likely to generate noise in thie process and that will make the decision boundary of SVM also learning from that noise. 

Interms of runtime, Multiple instance learning with bag dissimilarities is also a lot faster compared to mi_SVM since it does not require back and forth at the label of instance. Also, by computing the dissimilarities representation of input date we are able to decrease the size of input data as well. Both of these factors make dissimilarity approach run much faster compared to mi_SVM.

However, Multiple instance learning with bag dissimilarities is not the forever solution. It has a good performance in instance space dataset but it can not fully represent the relationship between different bags in attributed graph like the author has admitted. This lack of representation bility restricted the type of task that it can be employed. 

# Methods

## Multiple instance learning with bag dissimilarities (Veronika and David, 2015)
The method attempt to convert Multi instance learning dataset to normal dataset which can be used in regular learning algorithms. This is done by finding the dissimilarity representation for each of the bag $B_i$, $i \in {1..n}$. I have discribed the approach of turning the original data to dissimilarity representation above, but I will state it again in this section for convinience. 


The process of transforming original dataset into dissimilarities representation can be view as follow:
- compute a n\*n matrix M where n is the number of bags in the dataset
- $n_{ij} \in M$ denotes the dissimilarity between bag i and bag j

We can further break down this process into following steps:
- pick the interested instance in first bag $B_i$ where $i \in I$
- pick the interested instance in the second bag $B_j$ where $j \in I$
- Calculate the Hausdorff distance $d_h(B_i, B_j)=\max_k\min_l d(X_{i,k}, X_{j,l})$ between the two instance interested and use as $n_{ij}$
- repeat this process until the whole n\*n matrix is filled with dissimilarity representation

As stated ahead Hausdroff distance could be problematic when there is outliers in dataset. So we introduced a few new method which we use to substitute Hausdroff distance:
$$d_{minmin}(B_i, B_j) = \min_k\min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmin}(B_i, B_j) = \frac{1}{n_i}\sum_{k=1}^{n_i} \min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmean}(B_i, B_j) = \frac{1}{n_i n_j}\sum_{k=1}^{n_i} \sum_{l=1}^{n_j} d(X_{i,k}, X_{j,l})$$

The method that we employ to calculate the distance between two instance is Euclidean distance: $d(X_{i,k}, X_{j,l})$ stands for distance between $X_{i,k}$ and $X_{j,l}$ which is:
$$d(p,q)=\sqrt{\sum_{i=1}^n(q_i-p_i)^2}$$

After conversion, this n\*n dissimilarity representation matrix can be used in regular learning classifiers.


## Support Vector Machines for Multi ple-Instance Learning (Stuart, Ioannis, and Thomas, 2002)

This method mainly used the imbalence information provided by positive and negative bag to inference positive instance using the nagetive instance in nagetive bags. I have setup the equation that we want to optimize above. I am going to restate the equation for convinience and then talk about approach to optimize those equation.

### mi_SVM

$$\min_{y_i}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall i:y_i(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0, y_i \in {-1,1}$

Notice in a regular learning task the label $y_i$ will be given but for Multi instance learning taks label for $x_i$ from positive bag is an unknow variable. In mi-SVM one thus maximizes a soft-margin criterion jointly over possible label assignments as well as hyperplanes.

Steps take to optimize this equation:

initialize $y_i=Y_I \, for \, i \in I$
- Repeat
  - Compute SVM solution w, b for data set with imputed labels
  - Compute outputs $f_i=<w,x_i>+b$ for all $x_i$ in positive bags
  - Set $y_i=sign(f_i)$ for every $y_i, i\in I, Y_I=1$
  - For every positive bag $B_I$
    - If all instance been labeled -1
      - Compute $i=arg\max_{i\in I}f_i$ 
      - Set $y_i=1$
    - End
  - End
- While labels have changed



### MI_SVM

$$\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_I\xi_I$$

s.t. $\forall I:Y_I\max_{i\in{I}}(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=1$

$\forall i:-<w,x_i>+b \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=-1$

For positive bag we will introduce a selet variable $s(I)\in I$ denote the pattern been selected. Then we can rewrite the equation as:

$$\min_{s}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall I:Y_I\=-1 \wedge -<w,x_i>+b \geq 1-\xi_i, \forall i \in I$,

or, $\forall I:Y_I\=1 \wedge <w,x_i>+b \geq 1-\xi_i,$ and $\xi_I \geq 0$

Steps take to optimize this equation:

initialize $x_I=\sum_{i\in I}/|I|$ for every positive bag $B_I$
- Repeat
  - Compute QP solution w, b (the equation above) for data set with positive examples $x_I:Y=1$
  - Compute outputs $f_i=<w,x_i>+b$ for all $x_i$ in positive bags
  - Set $x_I=x_{s(I)}, s(I)=arg\max_{i\in I}f_i$, for every $I, Y_I=1$
- While selecter variable $s(I)$ have changed

# Research

## mi_SVM with Euclidean distance

The original multi instance learning support vector machine use the imbalence information provided by positive and negative bag to inference positive instance using the nagetive instance in nagetive bags. With this property it is actually using the instances from negative bag to correct the prediction of itself over time. The novel approach I have made is use the distance to find the selector variable which can represent the bag. Assume that instance is the one that has positive label and train the decision boundary using that information.

The extension decrese the runtime significantly. Before I retrofit my code to make it more efficient it could take hours to run, but with the extension it can finish in minutes. The improvement of the extension allow my algorithm not computing the argmax in the training epoch which saved a lot of time. I noticed from the result I have that this mechod is sensitive to outlier

## MI_SVM with mean feature as positive label

The origional MI_SVM which using the i that can give the maximum $f_i$ as the selected instance of the bag. In my approach, I assume that the distribution of positive and negative instance is far enough that a few positive instance can affect the mean of whole bag. Then I create positive instance by doing $x_I=\sum_{i\in I}/|I|$ for every positive bag $B_I$. 

Noticing that in the origional approach of MI_SVM, Stuart, Ioannis, and Thomas are not using the instance from negative bag. With my new extension, after creating positive instance using the information from positive bags, The instance from positive bags will not be used in the training. Instead, we will use the instance in the negative bag. 

The central idea of whole multi instance learning support vector machine is positive bag and negative bag provide imbalanced information. We know that all instance from negative bas are negative, thus we can use them to maximize the boundary of support vector machine.

# Results, Analysis and Discussion

## Accuracy (%)
| Dataset | MIL_dis_meanmean | MIL_dis_meanmin | MIL_dis_minmin | mi_SVM | MI_SVM | mi_Euclidean_SVM | MI_SVM_mean |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Musk1 | 85 $\pm$ 4.1 | **86 $\pm$ 11.0** | 83 $\pm$ 8.7 | 61 $\pm$ 7.2 | 67 $\pm$ 13.0 | 65 $\pm$ 12.0 | 63 $\pm$ 14.0 |
| Elephant | 82 $\pm$ 7.5 | 85 $\pm$ 5.9 | **87 $\pm$ 4.0** | 69 $\pm$ 2.0 | 60 $\pm$ 5.9 | 67 $\pm$ 5.8 | 69 $\pm$ 8.0 |

## Precision (%)
| Dataset | MIL_dis_meanmean | MIL_dis_meanmin | MIL_dis_minmin | mi_SVM | MI_SVM | mi_Euclidean_SVM | MI_SVM_mean |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Musk1 | **88 $\pm$ 6.7** | 85 $\pm$ 11.0 | 83 $\pm$ 11.0 | 66 $\pm$ 18.0 | 63 $\pm$ 11.0 | 62 $\pm$ 8.9 | 71 $\pm$ 32.0 |
| Elephant | 83 $\pm$ 9.4 | 86 $\pm$ 6.1 | **88 $\pm$ 4.8** | 62 $\pm$ 1.6 | 56 $\pm$ 3.6 | 61 $\pm$ 4.8 | 62 $\pm$ 6.5 |

## Recall (%)
| Dataset | MIL_dis_meanmean | MIL_dis_meanmin | MIL_dis_minmin | mi_SVM | MI_SVM | mi_Euclidean_SVM | MI_SVM_mean |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Musk1 | 83 $\pm$ 15.0 | 87 $\pm$ 11.0 | 85 $\pm$ 7.7 | 84 $\pm$ 31.0 | **100 $\pm$ 0.0** | 86 $\pm$ 20.0 | 47 $\pm$ 31.0 |
| Elephant | 81 $\pm$ 8.6 | 84 $\pm$ 7.3 | 86 $\pm$ 3.7 | 99 $\pm$ 2.0 | **100 $\pm$ 0.0** | 97 $\pm$ 2.4 | 99 $\pm$ 3.0 |

These result are aquired by running on Musk1 and Elephant dataset. The setting for the set was cross validation with fold size equals to 5 and random shuffle examples while creating folds.

Overall, the **dissimilarity method perform much better and stable** compared to multi instance learning support vector machine. The excellent 87 percent accuracy on Elephant and 86% accuracy on Musk1 proved the robustness of this dissimilarity representation. Also, interms of run time, dissimilarity method is more stable and efficient. It can compute the result within one minute for Musk1 dataset and less than one and a half minute on Elephant dataset. Compared to multi instance learning support vector machine which runtime can range from new minutes to more than hour, dissimilarity method seems to be a better choice.

The weeknees of dissimilarity method is that it requires researcher to setup different equations to get the dissimilarity representation respect to the type of data that was given. When the data given was a discription of distribution or an attribute graph, dissimilarity method requires extra computation effort and may not be able to fully represent the origional dataset.

For multi instance learning support vector machine, the major disadvantage is you need a very good starting point. By a very good starting point I mean the first epoch is very important. I printed out my gradient and I saw that for mi_SVM and MI_SVM, if the last few training example are those imputed example. Very likely that they are those negative example in the positive bag which been falsly labeled as positive during the setup phase. With those label used as the last few example to train the whole support vector machine, in the following examination phase, all instance will be classify as positive. Thus the algorithm will stop and return what it has. In ideal setting, there will be mixed imputed instance from positive bags and true negative instance from negative bags to correct the decision boundary. If that happened, the algorithm can run without error and have relatively high accuracy. That is also the reason for multi instance learning support vector machine having a relativevly higher standard deviation. 

My extension indeed working in this process and save runtime significantly. For most of my test runs, both of my research extension can finish computing within ten minutes. Compared to running time which can extend to an hour for the origional approach, that is a big progress. Also, at the same time, my research extension does not hurt the performance of the algorithm. However, they share the same problem with original multi instance learning support vector machine that they also need a good starting point.

For furture improvement of these methods, for dissimilarity method, I do not have that much to comment since it is both robust and efficent. For multi instance learning support vector machine, I would suggest whild doing cross validation, create the fold in order. If the example in the fold can appear in sequence which positive example and negative example show up repeatedly in order, I believe the performance of both mi_SVM and MI_SVM can be enhenced a lot. Since for now, some of the time they just stop at the first few epoch.

# Bibliography
J. Amores, "Multiple instance classification: Review, taxonomy and comparative study", in: Artificial Intelligence, Volume 201, 2013, pp. 81-105

V. Cheplygina, D. Tax, M. Loog, "Multiple instance learning with bag dissimilarities", in: Pattern Recognition, Volume 48, Issue 1, January 2015, Pages 264-275

S. Andrews, I. Tsochantaridis and T Hofmann, "Support Vector Machines for Multi ple-Instance Learning", 2002.

S. Huang, Z. Liu, W. Jin, Y. Mu, "Bag dissimilarity regularized multi-instance learning", in: Pattern Recognition, Volume 126, June 2022, 108583
