### CSDS 440 Final Project Individual Writeup

**Zaichuan You**

# Survey
For this project, I read the papers *"Multiple instance classification: Review, taxonomy and comparative study"* (J. Amores, 2013), *"Multiple instance learning with bag dissimilarities* (Veronika and David, 2015), and *"Support Vector Machines for Multi ple-Instance Learning"* (Stuart, Ioannis, and Thomas, 2002).

## Multiple instance classification: Review, taxonomy and comparative study (J. Amores, 2013)
This is the seed paper send out by Prof. Ray which give a insight of what is Multi instance learning. It has included the basis of Multiple instance learning and also different approaches that people has made to solve Multi instance learning problem. The basic assumption that all Multi instance learning problem share is:

- Bags are labeled either positive or negative and the label for each instance was not give.
- If a Bag contains at least one positive instance then it will be labeled positive, negative otherwise.

In mathmatical form, Multi instance learning problem can be discribe using Bag $X_i$ and instance $x_j \in X_i$. For ${x_1 .... x_j} \in X_i$ if any $x_n \in X_i$ is positive, $Y_i = 1$

This kind of problem was driven by the real world challenge of classifing images and molecues. In the paper Paulo has included few example to illustrate the basic setup for these problems. One classic example of Multi instance learning problem is the image of beach. In this example, the whole image was a Bag of input. Each instance was generated by kernal sliding through the image. The instance in this case was circled with red lines.

![WeChat Image_20221207215946](https://user-images.githubusercontent.com/89466889/206345550-32579999-3e46-44fa-acba-5df4bcd78b39.png)

The intuitive idea from this example is that it does not make sense if the classifier classify the second image as beach and said "I think this image is beach because it has a blue sky". In this case we know the whole Image was labeled as positive which indicates that it is beach but we can not tell which part of the image is the instance that tells us this is a beach.

This can also be simplify to instance space which we have only a few features instead of kernals as instance. In this example we can clearly see that each bag contains a selection of instance from the instance space. For each bag, if it contains at least one instance from the positive class the whole bag will be classified as positive.

![WeChat Image_20221207220522](https://user-images.githubusercontent.com/89466889/206346368-583915a5-ad2c-4c94-9d27-6aff4c833eea.png)

### Bag distance approach

In their work Paulo named the approaches which focus on learning the true label of each bag as Bag space approach. In Bag space approach researcher want their program to learn a decision boundary which can classify the true label of each bag. There is another paradigm called Embedded space but we will not talk about that today. 

In Bag space approaches, a few of them are focusing on the distance between each bag and using that as an indicator of the true label of each bag. One of the algorithm I have implmented from Veronika and David was employing this method. The bag distancce approach is turning the origional input dataset into a distance matrix which columns and rows are bags. 

![WeChat Image_20221207184027](https://user-images.githubusercontent.com/89466889/206347335-4e98d886-3cb5-439e-8643-68dc00d52273.png)

## Multiple instance learning with bag dissimilarities (Veronika and David, 2015)
This paper discribed the dissimilarities between bags which is calculated using the distance between bags. Bag dissimilarities is an extention of Citation-k NN proposed in 2014 by Veronika Cheplygina. It converge the Multi instance learning task to a normal learning problem by reducing the dimension of data. 

For Multi instance learning problem, instead of getting a dataset which is in standard feature vector representation such that regular supervised classifiers can be used, we we given the data in bag representation which does not include the true label for each instance in the positive bag. This stoping us from employing the regular supervised classifiers. 

The process of transforming original dataset into dissimilarities representation can be view as follow:
- compute a n\*n matrix M where n is the number of bags in the dataset
- $n_{ij} \in M$ denotes the dissimilarity between bag i and bag j

![WeChat Image_20221207223841](https://user-images.githubusercontent.com/89466889/206350859-56db7333-f836-48fa-9c7f-1163416915fa.png)

After transformation each row i can now be view as the list of feature for bag $B_i$ and label for bag $B_i$ is just $Y_i$. Now this matrix can be learned using regular learning approach such as Logistic Regression. 

Based on differen type of problems been given, the distribution of instance will change and we have to use different method to compute the dissimilarites between bags. For instance space which data given was a point set, dissimilarities is defined through the distance between bags. For distribution as instance, dissimilarities is defined though a distribution distance. For attribute graph where the instances are the nodes, andrelationships between instances are the edges. In this case,d is defined as a graph kernel or distance.

Since the dataset Musk1, Musk2, Elephant are all instance space dataset, I will only explain and implement the instance space approach in my project.

The approach of computing the distance between bags can be seen as follow:
- pick the interested instance in first bag $B_i$
- pick the interested instance in the second bag $B_j$
- find the distance between the two instance interested and use as $n_{ij}$

The method been employed here to calculate the distance is Hausdorff distance which $d_h(B_i, B_j)=\max_k\min_l d(X_{i,k}, X_{j,l})$. As $d_h$ is not symetric, the final Hausdorff distance is symmetrized by taking the maximum of the directed distances. All of these steps ensure that the Hausdorff distance is metric. The figure below showed the asymmetric property of $d_h$

![WeChat Image_20221207182642](https://user-images.githubusercontent.com/89466889/206355444-a0b328be-480d-440a-becc-63bbc01600bc.png)

However, Hausdorff distance is highly sensitive to outliers. To mitigate the effect of outlisers, the following change has been made upon Hausdorff distance:
$$d_{minmin}(B_i, B_j) = \min_k\min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmin}(B_i, B_j) = \frac{1}{n_i}\sum_{k=1}^{n_i} \min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmean}(B_i, B_j) = \frac{1}{n_i n_j}\sum_{k=1}^{n_i} \sum_{l=1}^{n_j} d(X_{i,k}, X_{j,l})$$

$d(X_{i,k}, X_{j,l})$ in this case is the Euclidean distance between $X_{i,k}$ and $X_{j,l}$ which is:
$$d(p,q)=\sqrt{\sum_{i=1}^n(q_i-p_i)^2}$$

After conversion, the input matrix will be turn into dissimilarity matrix which is n\*n

## Support Vector Machines for Multi ple-Instance Learning (Stuart, Ioannis, and Thomas, 2002)
This approach tries to learn the true label for each instance. To do that they used the imbalanced information provided by positive and negative bags to train the classifier. They will keep correcting the label of instance in positive bags using the instance from negative bag until there exist at least one instance in each positive bag which been classified as positive. 

### mi_SVM

$$\min_{y_i}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall i:y_i(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0, y_i \in {-1,1}$

Notice in a regular learning task the label $y_i$ will be given but for Multi instance learning taks label for $x_i$ from positive bag is an unknow variable. In mi-SVM one thus maximizes a soft-margin criterion jointly over possible label assignments as well as hyperplanes.

After wards this will be used to optimize the decision boundary and I will mention that in method section.

### MI_SVM

$$\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_I\xi_I$$

s.t. $\forall I:Y_I\max_{i\in{I}}(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=1$

$\forall i:-<w,x_i>+b \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=-1$

For positive bag we will introduce a selet variable $s(I)\in I$ denote the pattern been selected. Then we can rewrite the equation as:

$$\min_{s}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall I:Y_I\=-1 \wedge -<w,x_i>+b \geq 1-\xi_i, \forall i \in I$,

or, $\forall I:Y_I\=1 \wedge <w,x_i>+b \geq 1-\xi_i,$ and $\xi_I \geq 0$

## Comparason
As a whole, Multiple instance learning with bag dissimilarities out performed Support Vector Machines for Multi ple-Instance Learning. This is because leaning the specific label assigned to each instance could be more complecated than simply leang from bag label. With continuous making assumption of the label of instance inside the positive bag, it is likely multi instance support vector machine itself will create noise in thie process. The larger the dataset is, multi instance support vector machine is more likely to generate noise in thie process and that will make the decision boundary of SVM also learning from that noise. 

Interms of runtime, Multiple instance learning with bag dissimilarities is also a lot faster compared to mi_SVM since it does not require back and forth at the label of instance. Also, by computing the dissimilarities representation of input date we are able to decrease the size of input data as well. Both of these factors make dissimilarity approach run much faster compared to mi_SVM.

However, Multiple instance learning with bag dissimilarities is not the forever solution. It has a good performance in instance space dataset but it can not fully represent the relationship between different bags in attributed graph like the author has admitted. This lack of representation bility restricted the type of task that it can be employed. 

# Methods

## Multiple instance learning with bag dissimilarities (Veronika and David, 2015)
The method attempt to convert Multi instance learning dataset to normal dataset which can be used in regular learning algorithms. This is done by finding the dissimilarity representation for each of the bag $B_i$, $i \in {1..n}$. I have discribed the approach of turning the original data to dissimilarity representation above, but I will state it again in this section for convinience. 


The process of transforming original dataset into dissimilarities representation can be view as follow:
- compute a n\*n matrix M where n is the number of bags in the dataset
- $n_{ij} \in M$ denotes the dissimilarity between bag i and bag j

We can further break down this process into following steps:
- pick the interested instance in first bag $B_i$ where $i \in I$
- pick the interested instance in the second bag $B_j$ where $j \in I$
- Calculate the Hausdorff distance $d_h(B_i, B_j)=\max_k\min_l d(X_{i,k}, X_{j,l})$ between the two instance interested and use as $n_{ij}$
- repeat this process until the whole n\*n matrix is filled with dissimilarity representation

As stated ahead Hausdroff distance could be problematic when there is outliers in dataset. So we introduced a few new method which we use to substitute Hausdroff distance:
$$d_{minmin}(B_i, B_j) = \min_k\min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmin}(B_i, B_j) = \frac{1}{n_i}\sum_{k=1}^{n_i} \min_l d(X_{i,k}, X_{j,l})$$
$$d_{meanmean}(B_i, B_j) = \frac{1}{n_i n_j}\sum_{k=1}^{n_i} \sum_{l=1}^{n_j} d(X_{i,k}, X_{j,l})$$

The method that we employ to calculate the distance between two instance is Euclidean distance: $d(X_{i,k}, X_{j,l})$ stands for distance between $X_{i,k}$ and $X_{j,l}$ which is:
$$d(p,q)=\sqrt{\sum_{i=1}^n(q_i-p_i)^2}$$

After conversion, this n\*n dissimilarity representation matrix can be used in regular learning classifiers.


## Support Vector Machines for Multi ple-Instance Learning (Stuart, Ioannis, and Thomas, 2002)

This method mainly used the imbalence information provided by positive and negative bag to inference positive instance using the nagetive instance in nagetive bags. I have setup the equation that we want to optimize above. I am going to restate the equation for convinience and then talk about approach to optimize those equation.

### mi_SVM

$$\min_{y_i}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall i:y_i(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0, y_i \in {-1,1}$

Notice in a regular learning task the label $y_i$ will be given but for Multi instance learning taks label for $x_i$ from positive bag is an unknow variable. In mi-SVM one thus maximizes a soft-margin criterion jointly over possible label assignments as well as hyperplanes.

Steps take to optimize this equation:

initialize $y_i=Y_I \, for \, i \in I$
- Repeat
  - Compute SVM solution w, b for data set with imputed labels
  - Compute outputs $f_i=<w,x_i>+b$ for all $x_i$ in positive bags
  - Set $y_i=sign(f_i)$ for every $y_i, i\in I, Y_I=1$
  - For every positive bag $B_I$
    - If all instance been labeled -1
      - Compute $i=arg\max_{i\in I}f_i$ 
      - Set $y_i=1$
    - End
  - End
- While labels have changed



### MI_SVM

$$\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_I\xi_I$$

s.t. $\forall I:Y_I\max_{i\in{I}}(<w,x_i>+b) \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=1$

$\forall i:-<w,x_i>+b \geq 1-\xi_i, \xi_i \geq 0$ where $Y_I=-1$

For positive bag we will introduce a selet variable $s(I)\in I$ denote the pattern been selected. Then we can rewrite the equation as:

$$\min_{s}\min_{w,b,\xi}\frac{1}{2}||w||^2+C\sum_i\xi_i$$

s.t. $\forall I:Y_I\=-1 \wedge -<w,x_i>+b \geq 1-\xi_i, \forall i \in I$,

or, $\forall I:Y_I\=1 \wedge <w,x_i>+b \geq 1-\xi_i,$ and $\xi_I \geq 0$

Steps take to optimize this equation:

initialize $x_I=\sum_{i\in I}/|I|$ for every positive bag $B_I$
- Repeat
  - Compute QP solution w, b (the equation above) for data set with positive examples $x_I:Y=1$
  - Compute outputs $f_i=<w,x_i>+b$ for all $x_i$ in positive bags
  - Set $x_I=x_{s(I)}, s(I)=arg\max_{i\in I}f_i$, for every $I, Y_I=1$
- While selecter variable $s(I)$ have changed

# Research

## mi_SVM with Euclidean distance

The original multi instance learning support vector machine use the imbalence information provided by positive and negative bag to inference positive instance using the nagetive instance in nagetive bags. With this property it is actually using the instances from negativev bag to correct the prediction of itself over time. The novel approach I have made is use the distance to find the selector variable which can represent the bag. Assume that instance is the one that has positive label and train the decision boundary using that information.

The extension decrese the runtime significantly. Before I retrofit my code to make it more efficient it could take hours to run, but with the extension it can finish in minutes. The improvement of the extension allow my algorithm not computing the argmax in the training epoch which saved a lot of time. I noticed from the result I have that this mechod is sensitive to outlier

# Results, Analysis and Discussion

# Bibliography
J. Amores, "Multiple instance classification: Review, taxonomy and comparative study", in: Artificial Intelligence, Volume 201, 2013, pp. 81-105

V. Cheplygina, D. Tax, M. Loog, "Multiple instance learning with bag dissimilarities", in: Pattern Recognition, Volume 48, Issue 1, January 2015, Pages 264-275

S. Andrews, I. Tsochantaridis and T Hofmann, "Support Vector Machines for Multi ple-Instance Learning", 2002.

S. Huang, Z. Liu, W. Jin, Y. Mu, "Bag dissimilarity regularized multi-instance learning", in: Pattern Recognition, Volume 126, June 2022, 108583
